{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjkBUWm8ZMlc"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOTfaUaSZKfF"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZC64QGBZ2v9"
      },
      "source": [
        "# Gemini Quickstart: Gemini Robotics-ER 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMOdCB5FRVMZ"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/gemini-robotics-er.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDDzTW8DRVcc"
      },
      "source": [
        "This notebook introduces the **Gemini Robotics-ER 1.5** model.\n",
        "\n",
        "Gemini Robotics-ER 1.5 is a vision-language model (VLM) that brings Gemini's agentic capabilities to robotics. It's designed for advanced reasoning in the physical world, allowing robots to interpret complex visual data, perform spatial reasoning, and plan actions from natural language commands.\n",
        "\n",
        "Key features and benefits:\n",
        "\n",
        "* **Enhanced autonomy:** Robots can reason, adapt, and respond to changes in open-ended environments.\n",
        "* **Natural language interaction:** Makes robots easier to use by enabling complex task assignments using natural language.\n",
        "* **Task orchestration:** Deconstructs natural language commands into subtasks and integrates with existing robot controllers and behaviors to complete long-horizon tasks.\n",
        "* **Versatile capabilities:** Locates and identifies objects, understands object relationships, plans grasps and trajectories, and interprets dynamic scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jqmyhKZTZ1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section will need to be run any time you start up Colab. The example sections following this are intended to be able to be run without reliance on any other example section, so you may skip through to the examples that are most relevant/interesting to you at the time (though it is strongly recommended that you read through each of these examples at least once!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cdSPBCltkeQ"
      },
      "source": [
        "### Install SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ONn2UcU7akFl"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD3FmivAUXPk"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cells, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6L0iD346and2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dc6376c6-f32d-4da2-9348-0934387ad346"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GOOGLE_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1934857622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GOOGLE_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al7zVIf2UqqY"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "Initialize a Gemini SDK client with your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ezNXUK8arKa"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E3SYATU_u4"
      },
      "source": [
        "### Select the Gemini Robotics-ER 1.5 model and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIcki0r8VNHY"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-robotics-er-1.5-preview\"\n",
        "\n",
        "print(\n",
        "    client.models.generate_content(\n",
        "        model=MODEL_ID, contents=\"Are you there?\"\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-70SggjWURa"
      },
      "source": [
        "### Imports and utility code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12V7NJ6WlMV"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P37Dv6YQaYw9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgFZ72Bva1FU"
      },
      "source": [
        "#### Parsing JSON output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH6qyE1Uacoq"
      },
      "outputs": [],
      "source": [
        "def parse_json(json_output):\n",
        "  # Parsing out the markdown fencing\n",
        "  lines = json_output.splitlines()\n",
        "  for i, line in enumerate(lines):\n",
        "    if line == \"```json\":\n",
        "      # Remove everything before \"```json\"\n",
        "      json_output = \"\\n\".join(lines[i + 1 :])\n",
        "      # Remove everything after the closing \"```\"\n",
        "      json_output = json_output.split(\"```\")[0]\n",
        "      break  # Exit the loop once \"```json\" is found\n",
        "  return json_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRlsn7tJYUjI"
      },
      "source": [
        "#### Resize images\n",
        "\n",
        "Resize images for faster rendering and smaller API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMbTihD9YU9D"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_image_resized(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize(\n",
        "        (800, int(800 * img.size[1] / img.size[0])), Image.Resampling.LANCZOS\n",
        "    )\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuvfFW2sa1wu"
      },
      "source": [
        "#### Visualization helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "metin ve sayi\n",
        "\n"
      ],
      "metadata": {
        "id": "_yJl95K0GqAy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3991415e"
      },
      "source": [
        "display_gif(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b96c3d4"
      },
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-arms-table.png -O aloha-arms-table.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/gameboard.png -O gameboard.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/washers.png -O washer.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-pen.gif -O aloha-pen.gif -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4417dfb0"
      },
      "source": [
        "gif_path = \"aloha-pen.gif\"\n",
        "gif = Image.open(gif_path)\n",
        "display(gif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1e3f42d"
      },
      "source": [
        "img_path = \"gameboard.png\"\n",
        "img = get_image_resized(img_path)\n",
        "print(f\"Original size: {Image.open(img_path).size}, Resized size: {img.size}\")\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e296e438"
      },
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-arms-table.png -O aloha-arms-table.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/gameboard.png -O gameboard.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/washers.png -O washer.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-pen.gif -O aloha-pen.gif -q\n",
        "!wget h tt,ps://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha_desk.png -O aloha_desk.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/particles.jpg -O particles.jpg -q\n",
        "!wget https ://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/livingroom.jpeg -O livingroom.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/clear_space.png -O clear_space.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/desk_organization.mp4 -O desk_organization.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/bookshelf.jpeg -O bookshelf.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/cart.png -O cart.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/sockets.jpeg -O sockets.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/weights.jpeg -O weights.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/lunch.png -O lunch.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_1.png -O initial_state_1.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_2.png -O initial_state_2.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_3.png -O initial_state_3.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_4.png -O initial_state_4.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_1.png -O current_state_1.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_2.png -O current_state_2.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_3.png -O current_state_3.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_4.png -O current_state_4.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/air_quality.jpeg -O air_quality.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/mango.png -O mango.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/soarm-block.png -O soarm-block.png -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c9e79d8"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_image_resized(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize(\n",
        "        (800, int(800 * img.size[1] / img.size[0])), Image.Resampling.LANCZOS\n",
        "    )\n",
        "    return img\n",
        "\n",
        "img_path = \"aloha-arms-table.png\"\n",
        "img = get_image_resized(img_path)\n",
        "print(f\"Original size: {Image.open(img_path).size}, Resized size: {img.size}\")\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FULLZ\n",
        "pii\n"
      ],
      "metadata": {
        "id": "1NyGyEbuEPki"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4BQssBwEOzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0335dbd9"
      },
      "source": [
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQQ36nsiX0w-"
      },
      "outputs": [],
      "source": [
        "imp ort base64\n",
        "import dataclasses\n",
        "from io import BytesIO\n",
        "impolrt numpy as np\n",
        "from PIL import ImageColor, ImageDraw, ImageFont\n",
        "from typing import Tuple\n",
        "\n",
        "import IPython\n",
        "from IPython import display\n",
        "\n",
        "def generate_point_html(pil_image, points_json):\n",
        "  buffered = BytesIO()\n",
        "  pil_image.save(buffered, format=\"PNG\")\n",
        "  img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "  points_json = parse_json(points_json)\n",
        "\n",
        "  return f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Point Visualization</title>\n",
        "    <style>\n",
        "        body {{\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            background: #fff;\n",
        "            color: #000;\n",
        "            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n",
        "        }}\n",
        "\n",
        "        .point-overlay {{\n",
        "            position: absolute;\n",
        "            top: 0;\n",
        "            left: 0;\n",
        "            width: 100%;\n",
        "            height: 100%;\n",
        "            pointer-events: none;\n",
        "        }}\n",
        "\n",
        "        .point {{\n",
        "            position: absolute;\n",
        "            width: 12px;\n",
        "            height: 12px;\n",
        "            background-color: #2962FF;\n",
        "            border: 2px solid #fff;\n",
        "            border-radius: 50%;\n",
        "            transform: translate(-50%, -50%);\n",
        "            box-shadow: 0 0 40px rgba(41, 98, 255, 0.6);\n",
        "            opacity: 0;\n",
        "            transition: all 0.3s ease-in;\n",
        "            pointer-events: auto;\n",
        "        }}\n",
        "\n",
        "        .point.visible {{\n",
        "            opacity: 1;\n",
        "        }}\n",
        "\n",
        "        .point.fade-out {{\n",
        "            animation: pointFadeOut 0.3s forwards;\n",
        "        }}\n",
        "\n",
        "        .point.highlight {{\n",
        "            transform: translate(-50%, -50%) scale(1.1);\n",
        "            background-color: #FF4081;\n",
        "            box-shadow: 0 0 40px rgba(255, 64, 129, 0.6);\n",
        "            z-index: 100;\n",
        "        }}\n",
        "\n",
        "        @keyframes pointFadeOut {{\n",
        "            from {{\n",
        "                opacity: 1;\n",
        "            }}\n",
        "            to {{\n",
        "                opacity: 0.7;\n",
        "            }}\n",
        "        }}\n",
        "\n",
        "        .point-label {{\n",
        "            position: absolute;\n",
        "            background-color: #2962FF;\n",
        "            color: #fff;\n",
        "            font-size: 14px;\n",
        "            padding: 4px 12px;\n",
        "            border-radius: 4px;\n",
        "            transform: translate(20px, -10px);\n",
        "            white-space: nowrap;\n",
        "            opacity: 0;\n",
        "            transition: all 0.3s ease-in;\n",
        "            box-shadow: 0 0 30px rgba(41, 98, 255, 0.4);\n",
        "            pointer-events: auto;\n",
        "            cursor: pointer;\n",
        "        }}\n",
        "\n",
        "        .point-label.visible {{\n",
        "            opacity: 1;\n",
        "        }}\n",
        "\n",
        "        .point-label.fade-out {{\n",
        "            opacity: 0.45;\n",
        "        }}\n",
        "\n",
        "        .point-label.highlight {{\n",
        "            background-color: #FF4081;\n",
        "            box-shadow: 0 0 30px rgba(255, 64, 129, 0.4);\n",
        "            transform: translate(20px, -10px) scale(1.1);\n",
        "            z-index: 100;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div id=\"container\" style=\"position: relative;\">\n",
        "        <canvas id=\"canvas\" style=\"background: #000;\"></canvas>\n",
        "        <div id=\"pointOverlay\" class=\"point-overlay\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        function annotatePoints(frame) {{\n",
        "            // Add points with fade effect\n",
        "            const pointsData = {points_json};\n",
        "\n",
        "            const pointOverlay = document.getElementById(\"pointOverlay\");\n",
        "            pointOverlay.innerHTML = \"\";\n",
        "\n",
        "            const points = [];\n",
        "            const labels = [];\n",
        "\n",
        "            pointsData.forEach(pointData => {{\n",
        "                // Skip entries without coodinates.\n",
        "                if (!(pointData.hasOwnProperty(\"point\")))\n",
        "                  return;\n",
        "\n",
        "                const point = document.createElement(\"div\");\n",
        "                point.className = \"point\";\n",
        "                const [y, x] = pointData.point;\n",
        "                point.style.left = `${{x/1000.0 * 100.0}}%`;\n",
        "                point.style.top = `${{y/1000.0 * 100.0}}%`;\n",
        "\n",
        "                const pointLabel = document.createElement(\"div\");\n",
        "                pointLabel.className = \"point-label\";\n",
        "                pointLabel.textContent = pointData.label;\n",
        "                point.appendChild(pointLabel);\n",
        "\n",
        "                pointOverlay.appendChild(point);\n",
        "                points.push(point);\n",
        "                labels.push(pointLabel);\n",
        "\n",
        "                setTimeout(() => {{\n",
        "                    point.classList.add(\"visible\");\n",
        "                    pointLabel.classList.add(\"visible\");\n",
        "                }}, 0);\n",
        "\n",
        "                // Add hover effects\n",
        "                const handleMouseEnter = () => {{\n",
        "                    // Highlight current point and label\n",
        "                    point.classList.add(\"highlight\");\n",
        "                    pointLabel.classList.add(\"highlight\");\n",
        "\n",
        "                    // Fade out other points and labels\n",
        "                    points.forEach((p, idx) => {{\n",
        "                        if (p !== point) {{\n",
        "                            p.classList.add(\"fade-out\");\n",
        "                            labels[idx].classList.add(\"fade-out\");\n",
        "                        }}\n",
        "                    }});\n",
        "                }};\n",
        "\n",
        "                const handleMouseLeave = () => {{\n",
        "                    // Remove highlight from current point and label\n",
        "                    point.classList.remove(\"highlight\");\n",
        "                    pointLabel.classList.remove(\"highlight\");\n",
        "\n",
        "                    // Restore other points and labels\n",
        "                    points.forEach((p, idx) => {{\n",
        "                        p.classList.remove(\"fade-out\");\n",
        "                        labels[idx].classList.remove(\"fade-out\");\n",
        "                    }});\n",
        "                }};\n",
        "\n",
        "                point.addEventListener(\"mouseenter\", handleMouseEnter);\n",
        "                point.addEventListener(\"mouseleave\", handleMouseLeave);\n",
        "                pointLabel.addEventListener(\"mouseenter\", handleMouseEnter);\n",
        "                pointLabel.addEventListener(\"mouseleave\", handleMouseLeave);\n",
        "            }});\n",
        "        }}\n",
        "\n",
        "        // Initialize canvas\n",
        "        const canvas = document.getElementById(\"canvas\");\n",
        "        const ctx = canvas.getContext(\"2d\");\n",
        "        const container = document.getElementById(\"container\");\n",
        "\n",
        "        // Load and draw the image\n",
        "        const img = new Image();\n",
        "        img.onload = () => {{\n",
        "            const aspectRatio = img.height / img.width;\n",
        "            canvas.width = 800;\n",
        "            canvas.height = Math.round(800 * aspectRatio);\n",
        "            container.style.width = canvas.width + \"px\";\n",
        "            container.style.height = canvas.height + \"px\";\n",
        "\n",
        "            ctx.drawImage(img, 0, 0, canvas.width, canvas.height);\n",
        "\n",
        "            frame.width = canvas.width;\n",
        "            frame.height = canvas.height;\n",
        "            annotatePoints(frame);\n",
        "        }};\n",
        "        img.src = \"data:image/png;base64,{img_str}\";\n",
        "\n",
        "        const frame = {{\n",
        "            width: canvas.width,\n",
        "            height: canvas.height\n",
        "        }};\n",
        "\n",
        "        annotatePoints(frame);\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "additional_colors = [\n",
        "    colorname for (colorname, colorcode) in ImageColor.colormap.items()\n",
        "]\n",
        "\n",
        "\n",
        "def plot_bounding_boxes(img, bounding_boxes):\n",
        "  \"\"\"Plots bounding boxes on an image.\n",
        "\n",
        "  Plots bounding boxes on an image with markers for each a name, using PIL,\n",
        "  normalized coordinates, and different colors.\n",
        "\n",
        "  Args:\n",
        "      img_path: The path to the image file.\n",
        "      bounding_boxes: A list of bounding boxes containing the name of the object\n",
        "        and their positions in normalized [y1 x1 y2 x2] format.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the image\n",
        "  width, height = img.size\n",
        "  print(img.size)\n",
        "  # Create a drawing object\n",
        "  draw = ImageDraw.Draw(img)\n",
        "\n",
        "  # Define a list of colors\n",
        "  colors = [\n",
        "      \"red\",\n",
        "      \"green\",\n",
        "      \"blue\",\n",
        "      \"yellow\",\n",
        "      \"orange\",\n",
        "      \"pink\",\n",
        "      \"purple\",\n",
        "      \"brown\",\n",
        "      \"gray\",\n",
        "      \"beige\",\n",
        "      \"turquoise\",\n",
        "      \"cyan\",\n",
        "      \"magenta\",\n",
        "      \"lime\",\n",
        "      \"navy\",\n",
        "      \"maroon\",\n",
        "      \"teal\",\n",
        "      \"olive\",\n",
        "      \"coral\",\n",
        "      \"lavender\",\n",
        "      \"violet\",\n",
        "      \"gold\",\n",
        "      \"silver\",\n",
        "  ] + additional_colors\n",
        "\n",
        "  # Parsing out the markdown fencing\n",
        "  bounding_boxes = parse_json(bounding_boxes)\n",
        "\n",
        "  font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", size=14)\n",
        "\n",
        "  # Iterate over the bounding boxes\n",
        "  for i, bounding_box in enumerate(json.loads(bounding_boxes)):\n",
        "    # Select a color from the list\n",
        "    color = colors[i % len(colors)]\n",
        "\n",
        "    # Convert normalized coordinates to absolute coordinates\n",
        "    abs_y1 = int(bounding_box[\"box_2d\"][0] / 1000 * height)\n",
        "    abs_x1 = int(bounding_box[\"box_2d\"][1] / 1000 * width)\n",
        "    abs_y2 = int(bounding_box[\"box_2d\"][2] / 1000 * height)\n",
        "    abs_x2 = int(bounding_box[\"box_2d\"][3] / 1000 * width)\n",
        "\n",
        "    if abs_x1 > abs_x2:\n",
        "      abs_x1, abs_x2 = abs_x2, abs_x1\n",
        "\n",
        "    if abs_y1 > abs_y2:\n",
        "      abs_y1, abs_y2 = abs_y2, abs_y1\n",
        "\n",
        "    # Draw the bounding box\n",
        "    draw.rectangle(((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4)\n",
        "\n",
        "    # Draw the text\n",
        "    if \"label\" in bounding_box:\n",
        "      draw.text(\n",
        "          (abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font\n",
        "      )\n",
        "\n",
        "  # Display the image\n",
        "  img.show()\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class SegmentationMask:\n",
        "  # bounding box pixel coordinates (not normalized)\n",
        "  y0: int  # in [0..height - 1]\n",
        "  x0: int  # in [0..width - 1]\n",
        "  y1: int  # in [0..height - 1]\n",
        "  x1: int  # in [0..width - 1]\n",
        "  mask: np.array  # [img_height, img_width] with values 0..255\n",
        "  label: str\n",
        "\n",
        "\n",
        "def parse_segmentation_masks(\n",
        "    predicted_str: str, *, img_height: int, img_width: int\n",
        ") -> list[SegmentationMask]:\n",
        "  items = json.loads(parse_json(predicted_str))\n",
        "  masks = []\n",
        "  for item in items:\n",
        "    raw_box = item[\"box_2d\"]\n",
        "    abs_y0 = int(item[\"box_2d\"][0] / 1000 * img_height)\n",
        "    abs_x0 = int(item[\"box_2d\"][1] / 1000 * img_width)\n",
        "    abs_y1 = int(item[\"box_2d\"][2] / 1000 * img_height)\n",
        "    abs_x1 = int(item[\"box_2d\"][3] / 1000 * img_width)\n",
        "    if abs_y0 >= abs_y1 or abs_x0 >= abs_x1:\n",
        "      print(\"Invalid bounding box\", item[\"box_2d\"])\n",
        "      continue\n",
        "    label = item[\"label\"]\n",
        "    png_str = item[\"mask\"]\n",
        "    if not png_str.startswith(\"data:image/png;base64,\"):\n",
        "      print(\"Invalid mask\")\n",
        "      continue\n",
        "    png_str = png_str.removeprefix(\"data:image/png;base64,\")\n",
        "    png_str = base64.b64decode(png_str)\n",
        "    mask = Image.open(BytesIO(png_str))\n",
        "    bbox_height = abs_y1 - abs_y0\n",
        "    bbox_width = abs_x1 - abs_x0\n",
        "    if bbox_height < 1 or bbox_width < 1:\n",
        "      print(\"Invalid bounding box\")\n",
        "      continue\n",
        "    mask = mask.resize(\n",
        "        (bbox_width, bbox_height), resample=Image.Resampling.BILINEAR\n",
        "    )\n",
        "    np_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "    np_mask[abs_y0:abs_y1, abs_x0:abs_x1] = mask\n",
        "    masks.append(\n",
        "        SegmentationMask(abs_y0, abs_x0, abs_y1, abs_x1, np_mask, label)\n",
        "    )\n",
        "  return masks\n",
        "\n",
        "\n",
        "def overlay_mask_on_img(\n",
        "    img: Image, mask: np.ndarray, color: str, alpha: float = 0.7\n",
        ") -> Image.Image:\n",
        "  \"\"\"Overlays a single mask onto a PIL Image using a named color.\n",
        "\n",
        "  The mask image defines the area to be colored. Non-zero pixels in the\n",
        "  mask image are considered part of the area to overlay.\n",
        "\n",
        "  Args:\n",
        "      img: The base PIL Image object.\n",
        "      mask: A PIL Image object representing the mask. Should have the same\n",
        "        height and width as the img. Modes '1' (binary) or 'L' (grayscale) are\n",
        "        typical, where non-zero pixels indicate the masked area.\n",
        "      color: A standard color name string (e.g., 'red', 'blue', 'yellow').\n",
        "      alpha: The alpha transparency level for the overlay (0.0 fully\n",
        "        transparent, 1.0 fully opaque). Default is 0.7 (70%).\n",
        "\n",
        "  Returns:\n",
        "      A new PIL Image object (in RGBA mode) with the mask overlaid.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If color name is invalid, mask dimensions mismatch img\n",
        "                  dimensions, or alpha is outside the 0.0-1.0 range.\n",
        "  \"\"\"\n",
        "  if not (0.0 <= alpha <= 1.0):\n",
        "    raise ValueError(\"Alpha must be between 0.0 and 1.0\")\n",
        "\n",
        "  # Convert the color name string to an RGB tuple\n",
        "  try:\n",
        "    color_rgb: Tuple[int, int, int] = ImageColor.getrgb(color)\n",
        "  except ValueError as e:\n",
        "    # Re-raise with a more informative message if color name is invalid\n",
        "    raise ValueError(\n",
        "        f\"Invalid color name '{color}'. Supported names are typically HTML/CSS \"\n",
        "        f\"color names. Error: {e}\"\n",
        "    )\n",
        "\n",
        "  # Prepare the base image for alpha compositing\n",
        "  img_rgba = img.convert(\"RGBA\")\n",
        "  width, height = img_rgba.size\n",
        "\n",
        "  # Create the colored overlay layer\n",
        "  # Calculate the RGBA tuple for the overlay color\n",
        "  alpha_int = int(alpha * 255)\n",
        "  overlay_color_rgba = color_rgb + (alpha_int,)\n",
        "\n",
        "  # Create an RGBA layer (all zeros = transparent black)\n",
        "  colored_mask_layer_np = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "  # Mask has values between 0 and 255, threshold at 127 to get binary mask.\n",
        "  mask_np_logical = mask > 127\n",
        "\n",
        "  # Apply the overlay color RGBA tuple where the mask is True\n",
        "  colored_mask_layer_np[mask_np_logical] = overlay_color_rgba\n",
        "\n",
        "  # Convert the NumPy layer back to a PIL Image\n",
        "  colored_mask_layer_pil = Image.fromarray(colored_mask_layer_np, \"RGBA\")\n",
        "\n",
        "  # Composite the colored mask layer onto the base image\n",
        "  result_img = Image.alpha_composite(img_rgba, colored_mask_layer_pil)\n",
        "\n",
        "  return result_img\n",
        "\n",
        "\n",
        "def plot_segmentation_masks(\n",
        "    img: Image, segmentation_masks: list[SegmentationMask]\n",
        "):\n",
        "  \"\"\"Plots bounding boxes on an image.\n",
        "\n",
        "  Plots bounding boxes on an image with markers for each a name, using PIL,\n",
        "  normalized coordinates, and different colors.\n",
        "\n",
        "  Args:\n",
        "      img: The PIL.Image.\n",
        "      segmentation_masks: A string encoding as JSON a list of segmentation masks\n",
        "        containing the name of the object, their positions in normalized [y1 x1\n",
        "        y2 x2] format, and the png encoded segmentation mask.\n",
        "  \"\"\"\n",
        "  # Define a list of colors\n",
        "  colors = [\n",
        "      \"red\",\n",
        "      \"green\",\n",
        "      \"blue\",\n",
        "      \"yellow\",\n",
        "      \"orange\",\n",
        "      \"pink\",\n",
        "      \"purple\",\n",
        "      \"brown\",\n",
        "      \"gray\",\n",
        "      \"beige\",\n",
        "      \"turquoise\",\n",
        "      \"cyan\",\n",
        "      \"magenta\",\n",
        "      \"lime\",\n",
        "      \"navy\",\n",
        "      \"maroon\",\n",
        "      \"teal\",\n",
        "      \"olive\",\n",
        "      \"coral\",\n",
        "      \"lavender\",\n",
        "      \"violet\",\n",
        "      \"gold\",\n",
        "      \"silver\",\n",
        "  ] + additional_colors\n",
        "\n",
        "  font = ImageFont.load_default()\n",
        "\n",
        "  # Do this in 3 passes to make sure the boxes and text are always visible.\n",
        "\n",
        "  # Overlay the mask\n",
        "  for i, mask in enumerate(segmentation_masks):\n",
        "    color = colors[i % len(colors)]\n",
        "    img = overlay_mask_on_img(img, mask.mask, color)\n",
        "\n",
        "  # Create a drawing object\n",
        "  draw = ImageDraw.Draw(img)\n",
        "\n",
        "  # Draw the bounding boxes\n",
        "  for i, mask in enumerate(segmentation_masks):\n",
        "    color = colors[i % len(colors)]\n",
        "    draw.rectangle(\n",
        "        ((mask.x0, mask.y0), (mask.x1, mask.y1)), outline=color, width=4\n",
        "    )\n",
        "\n",
        "  # Draw the text labels\n",
        "  for i, mask in enumerate(segmentation_masks):\n",
        "    color = colors[i % len(colors)]\n",
        "    if mask.label != \"\":\n",
        "      draw.text((mask.x0 + 8, mask.y0 - 20), mask.label, fill=color, font=font)\n",
        "  return img\n",
        "\n",
        "\n",
        "def overlay_points_on_frames(original_frames, points_data_per_frame):\n",
        "  \"\"\"Overlays points on original frames and returns the modified frames.\"\"\"\n",
        "  modified_frames = []\n",
        "\n",
        "  # Define colors for drawing points (using a consistent color per label for clarity)\n",
        "  label_colors = {}\n",
        "  current_color_index = 0\n",
        "  available_colors = [\n",
        "      \"red\",\n",
        "      \"green\",\n",
        "      \"blue\",\n",
        "      \"yellow\",\n",
        "      \"orange\",\n",
        "      \"pink\",\n",
        "      \"purple\",\n",
        "      \"brown\",\n",
        "      \"gray\",\n",
        "      \"beige\",\n",
        "      \"turquoise\",\n",
        "      \"cyan\",\n",
        "      \"magenta\",\n",
        "      \"lime\",\n",
        "      \"navy\",\n",
        "      \"maroon\",\n",
        "      \"teal\",\n",
        "      \"olive\",\n",
        "      \"coral\",\n",
        "      \"lavender\",\n",
        "      \"violet\",\n",
        "      \"gold\",\n",
        "      \"silver\",\n",
        "  ]\n",
        "\n",
        "  font = ImageFont.load_default()\n",
        "\n",
        "  # Check if the number of original frames matches the number of processed data entries\n",
        "  if len(original_frames) != len(points_data_per_frame):\n",
        "    print(\n",
        "        f\"Error: Number of original frames ({len(original_frames)}) does not \"\n",
        "        \"match the number of processed point data entries\"\n",
        "        f\" ({len(points_data_per_frame)}). Cannot overlay points accurately.\"\n",
        "    )\n",
        "    return original_frames  # Return original frames if data doesn't match\n",
        "  else:\n",
        "    # Iterate through the frames and draw points\n",
        "    for i, frame_pil in enumerate(original_frames):\n",
        "      # Ensure frame is in RGB mode for drawing\n",
        "      img = frame_pil.convert(\"RGB\")\n",
        "      draw = ImageDraw.Draw(img)\n",
        "      width, height = img.size\n",
        "\n",
        "      frame_points = points_data_per_frame[i]\n",
        "\n",
        "      # Draw points on the frame\n",
        "      for point_info in frame_points:\n",
        "        if \"point\" in point_info and \"label\" in point_info:\n",
        "          y_norm, x_norm = point_info[\"point\"]\n",
        "          label = point_info[\"label\"]\n",
        "\n",
        "          # Get color for the label\n",
        "          if label not in label_colors:\n",
        "            label_colors[label] = available_colors[\n",
        "                current_color_index % len(available_colors)\n",
        "            ]\n",
        "            current_color_index += 1\n",
        "          color = label_colors[label]\n",
        "\n",
        "          # Convert normalized coordinates to absolute pixel coordinates\n",
        "          abs_x = int(x_norm / 1000.0 * width)\n",
        "          abs_y = int(y_norm / 1000.0 * height)\n",
        "\n",
        "          # Draw a circle at the point\n",
        "          point_radius = 5\n",
        "          draw.ellipse(\n",
        "              (\n",
        "                  abs_x - point_radius,\n",
        "                  abs_y - point_radius,\n",
        "                  abs_x + point_radius,\n",
        "                  abs_y + point_radius,\n",
        "              ),\n",
        "              fill=color,\n",
        "              outline=color,\n",
        "          )\n",
        "\n",
        "          # Draw the label\n",
        "          # Adjust label position to avoid going out of bounds\n",
        "          label_pos_x = abs_x + point_radius + 2\n",
        "          label_pos_y = (\n",
        "              abs_y - point_radius - 10\n",
        "              if abs_y - point_radius - 10 > 0\n",
        "              else abs_y + point_radius + 2\n",
        "          )\n",
        "          draw.text((label_pos_x, label_pos_y), label, fill=color, font=font)\n",
        "\n",
        "      # Append the modified PIL Image\n",
        "      modified_frames.append(img)\n",
        "\n",
        "    print(f\"Processed and drew points on {len(modified_frames)} frames.\")\n",
        "    return modified_frames\n",
        "\n",
        "\n",
        "def display_gif(frames_to_display):\n",
        "  \"\"\"Saves and displays a list of PIL Images as a GIF.\"\"\"\n",
        "  if frames_to_display:\n",
        "    try:\n",
        "      # Save the modified frames as a new GIF\n",
        "      output_gif_path = \"/tmp/annotated_aloha_pen.gif\"\n",
        "      # Duration per frame in milliseconds (adjust as needed, 40ms is 25fps)\n",
        "      duration_ms = 40\n",
        "      # Ensure all frames are in RGB mode before saving as GIF\n",
        "      rgb_frames = [frame.convert(\"RGB\") for frame in frames_to_display]\n",
        "      if rgb_frames:\n",
        "        rgb_frames[0].save(\n",
        "            output_gif_path,\n",
        "            save_all=True,\n",
        "            append_images=rgb_frames[1:],\n",
        "            duration=duration_ms,\n",
        "            loop=0,\n",
        "        )\n",
        "\n",
        "        # Display the GIF in Colab\n",
        "        display.display(display.Image(output_gif_path))\n",
        "        print(f\"Displayed annotated GIF: {output_gif_path}\")\n",
        "      else:\n",
        "        print(\"No frames to create GIF.\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error creating or displaying annotated GIF: {e}\")\n",
        "  else:\n",
        "    print(\"No frames to display.\")\n",
        "\n",
        "\n",
        "def extract_frames(gif):\n",
        "  \"\"\"Extracts frames from a GIF and returns a list of PIL Image objects.\"\"\"\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      # Convert each frame to RGB to ensure compatibility with drawing\n",
        "      frame = gif.convert(\"RGB\")\n",
        "      frames.append(frame)\n",
        "      gif.seek(gif.tell() + 1)  # Move to the next frame\n",
        "  except EOFError:\n",
        "    pass  # End of sequence\n",
        "\n",
        "  print(f\"Extracted {len(frames)} frames from the GIF.\")\n",
        "\n",
        "  return frames\n",
        "\n",
        "\n",
        "def populate_points_for_all_frames(total_frames, step, analyzed_data):\n",
        "  \"\"\"Populates point data for all frames based on analyzed frames.\"\"\"\n",
        "  points_data_all_frames = []\n",
        "  analyzed_data_index = 0\n",
        "  for i in range(total_frames):\n",
        "    if i % step == 0 and analyzed_data_index < len(analyzed_data):\n",
        "      points_data_all_frames.append(analyzed_data[analyzed_data_index])\n",
        "      analyzed_data_index += 1\n",
        "    else:\n",
        "      # For frames that were not analyzed, use the data from the last analyzed\n",
        "      # frame or append an empty list if no frame has been analyzed yet\n",
        "      if analyzed_data_index > 0:\n",
        "        points_data_all_frames.append(analyzed_data[analyzed_data_index - 1])\n",
        "      else:\n",
        "        # Should not happen if frames list is not empty\n",
        "        points_data_all_frames.append([])\n",
        "  return points_data_all_frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c008a469"
      },
      "source": [
        "frames = extract_frames(gif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytVkhMqtfnN"
      },
      "source": [
        "## Common settings\n",
        "\n",
        "Many of the examples work well with thinking turned off to reduce latency. Temperature is set to `0.5` to increase consistency while still allowing some creativity.\n",
        "\n",
        "Most of the examples use a common recipe to call the model - an image, a prompt, and a `GenerateContentConfig`. This function reduces code duplication and highlights when different settings are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHcUncwNtL-v"
      },
      "outputs": [],
      "source": [
        "def call_gemini_robotics_er(img, prompt, config=None):\n",
        "    default_config = types.GenerateContentConfig(\n",
        "        temperature=0.5,\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "    )\n",
        "\n",
        "    if config is None:\n",
        "        config = default_config\n",
        "\n",
        "    image_response = client.models.generate_content(\n",
        "          model=MODEL_ID,\n",
        "          contents=[img, prompt],\n",
        "          config=config,\n",
        "    )\n",
        "\n",
        "    print(image_response.text)\n",
        "    return parse_json(image_response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPDvMJz6dGhd"
      },
      "source": [
        "## 2D Pointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KBX_F0adKQ5"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-arms-table.png -O aloha-arms-table.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/gameboard.png -O gameboard.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/washers.png -O washer.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-pen.gif -O aloha-pen.gif -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbCHnSG3i_E0"
      },
      "source": [
        "### Pointing to Undefined Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqFpIQbtdbjE"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Point to no more than 10 items in the image. The label returned should be an\n",
        "    identifying name for the object detected.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnSGgHMyjEx4"
      },
      "source": [
        "### Pointing to Defined Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88XlG1f8-uH_"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "\n",
        "queries = [\n",
        "    \"bread\",\n",
        "    \"starfruit\",\n",
        "    \"banana\",\n",
        "]\n",
        "\n",
        "prompt = textwrap.dedent(f\"\"\"\\\n",
        "Get all points matching the following objects: {', '.join(queries)}. The label\n",
        "returned should be an identifying name for the object detected.\n",
        "\n",
        "The answer should follow the JSON format:\n",
        "[{{\"point\": <point>, \"label\": <label1>}}, ...]\n",
        "\n",
        "The points are in [y, x] format normalized to 0-1000.\n",
        "\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "points_data = []\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  points_data.extend(data)\n",
        "except json.JSONDecodeError:\n",
        "  print(\"Warning: Invalid JSON response. Skipping.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0mLsgdjPy9"
      },
      "source": [
        "### Point to all instances of an object based on more abstract description (e.g. \"fruit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vdb0xW3djfd"
      },
      "outputs": [],
      "source": [
        "points_data = []\n",
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "\n",
        "prompt = textwrap.dedent(f\"\"\"\\\n",
        "        Get all points for fruit. The label returned should be an identifying\n",
        "        name for the object detected.\n",
        "\n",
        "        The answer should follow the json format:\n",
        "        [{{\"point\": <point>, \"label\": <label1>}}, ...]\n",
        "\n",
        "        The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  points_data.extend(data)\n",
        "except json.JSONDecodeError:\n",
        "  print(f\"Warning: Invalid JSON response, skipping.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXI29BZNjTfM"
      },
      "source": [
        "### Point to all instances of an object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqapTqtbdw3c"
      },
      "outputs": [],
      "source": [
        "points_data = []\n",
        "img = get_image_resized(\"gameboard.png\")\n",
        "\n",
        "queries = [\n",
        "    \"game board slot\",\n",
        "    \"X game piece\",\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "for obj in queries:\n",
        "  prompt = textwrap.dedent(f\"\"\"\\\n",
        "      Get all points matching {obj}. The label returned should be an identifying\n",
        "      name for the object detected.\n",
        "\n",
        "      The answer should follow the JSON format:\n",
        "      [{{\"point\": <point>, \"label\": <label1>}}, ...]\n",
        "\n",
        "      The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "  json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "  try:\n",
        "    data = json.loads(json_output)\n",
        "    points_data.extend(data)\n",
        "  except json.JSONDecodeError:\n",
        "    print(f\"Warning: Invalid JSON response for {obj}. Skipping.\")\n",
        "    continue\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_w8K-ayjWYM"
      },
      "source": [
        "### Pointing to certain parts of an object in serial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3jMpkXndyfa"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "points_data = []\n",
        "\n",
        "queries = [\n",
        "    (\"paper bag\", \"handles\"),\n",
        "    (\"banana\", \"the stem\"),\n",
        "    (\"banana\", \"center\"),\n",
        "    (\"starfruit\", \"center\"),\n",
        "    (\"lime\", \"center\"),\n",
        "    (\"light blue bowl\", \"rim\"),\n",
        "    (\"dark blue bowl\", \"rim\"),\n",
        "    (\"measuring cup\", \"rim\"),\n",
        "    (\"measuring cup\", \"handle\"),\n",
        "    (\"bowl\", \"tomato\"),\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "for obj, part in queries:\n",
        "  POINT_PROMPT_TEMPLATE = textwrap.dedent(\"\"\"\\\n",
        "    Point to the $part of the $object in the image. Return the answer as a\n",
        "    JSON list of a dictionary with keys 'point' and 'label'. Only return one\n",
        "    point for this request.\"\"\")\n",
        "  prompt = POINT_PROMPT_TEMPLATE.replace(\"$object\", obj).replace(\"$part\", part)\n",
        "\n",
        "  json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "  try:\n",
        "    data = json.loads(json_output)\n",
        "    points_data.extend(data)\n",
        "  except json.JSONDecodeError:\n",
        "    print(f\"Warning: Invalid JSON response for {obj}, {part}. Skipping.\")\n",
        "    continue\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thIJjmAKd0yG"
      },
      "source": [
        "As you can see in the example above, pointing to these objects in a loop can take a moment (e.g. 15 seconds). When possible, it is recommended that you run your queries in parallel to improve response time, as you can see in the following example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGXj_wrejZnG"
      },
      "source": [
        "### Pointing to certain parts of an object in parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmLtTiWSd16y"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "\n",
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "points_data = []\n",
        "\n",
        "queries = [\n",
        "    (\"paper bag\", \"handles\"),\n",
        "    (\"banana\", \"the stem\"),\n",
        "    (\"banana\", \"center\"),\n",
        "    (\"starfruit\", \"center\"),\n",
        "    (\"lime\", \"center\"),\n",
        "    (\"light blue bowl\", \"rim\"),\n",
        "    (\"dark blue bowl\", \"rim\"),\n",
        "    (\"measuring cup\", \"rim\"),\n",
        "    (\"measuring cup\", \"handle\"),\n",
        "    (\"bowl\", \"tomato\"),\n",
        "]\n",
        "\n",
        "def process_query(obj, part):\n",
        "  POINT_PROMPT_TEMPLATE = textwrap.dedent(\"\"\"\\\n",
        "    Point to the $part of the $object in the image. Return the answer as a\n",
        "    JSON list of a dictionary with keys 'point' and 'label'. Only return one\n",
        "    point for this request.\"\"\")\n",
        "  prompt = POINT_PROMPT_TEMPLATE.replace(\"$object\", obj).replace(\"$part\", part)\n",
        "  json_output = call_gemini_robotics_er(img, prompt)\n",
        "  try:\n",
        "    data = json.loads(json_output)\n",
        "    return data\n",
        "  except json.JSONDecodeError:\n",
        "    print(f\"Warning: Invalid JSON response for {obj}, {part}. Skipping.\")\n",
        "    return []\n",
        "\n",
        "start_time = time.time()\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  results = executor.map(\n",
        "      lambda query: process_query(query[0], query[1]), queries\n",
        "  )\n",
        "\n",
        "for result in results:\n",
        "  points_data.extend(result)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88zkaMyajc4c"
      },
      "outputs": [],
      "source": [
        "### Counting by Pointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O8QCm1Pd3le"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"washer.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Point to each washer in the box. Return the answer in the format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  print(f\"count: {len(data)}\")\n",
        "except json.JSONDecodeError:\n",
        "  print(\"Error: Could not decode JSON response from the model.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoe0mV5Ujgg8"
      },
      "source": [
        "### Pointing to Defined Objects in a GIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah7f2SZuYZYi"
      },
      "outputs": [],
      "source": [
        "gif_path = \"aloha-pen.gif\"\n",
        "try:\n",
        "  gif = Image.open(gif_path)\n",
        "  print(f\"Successfully loaded GIF: {gif_path}\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: GIF file not found at {gif_path}\")\n",
        "  raise\n",
        "\n",
        "frames = extract_frames(gif)\n",
        "\n",
        "# Define the objects to query\n",
        "queries = [\n",
        "    \"pen (on desk)\",\n",
        "    \"pen (in robot hand)\",\n",
        "    \"laptop (opened)\",\n",
        "    \"laptop (closed)\",\n",
        "]\n",
        "\n",
        "prompt = textwrap.dedent(f\"\"\"\\\n",
        "Point to the following objects in the provided image: {\", \".join(queries)}.\n",
        "\n",
        "The answer should follow the JSON format:\n",
        "[{{\"point\": <point>, \"label\": <label1>}}, ...]\n",
        "\n",
        "The points are in [y, x] format normalized to 0-1000.\n",
        "\n",
        "If no objects are found, return an empty JSON list [].\"\"\")\n",
        "\n",
        "\n",
        "# Send every 10th frame as a separate request for the sake of time\n",
        "analyzed_frames_data = []\n",
        "frame_step = 10\n",
        "\n",
        "for i in range(0, len(frames), frame_step):\n",
        "  frame_index = i\n",
        "  frame = frames[frame_index]\n",
        "  print(f\"Processing frame {frame_index+1}/{len(frames)}...\")\n",
        "\n",
        "  try:\n",
        "    image_response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[frame, prompt],\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.5,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    try:\n",
        "      json_output = parse_json(image_response.text)\n",
        "      frame_points = json.loads(json_output)\n",
        "      analyzed_frames_data.append(frame_points)\n",
        "      print(\n",
        "          f\"  Successfully parsed {len(frame_points)} points for frame\"\n",
        "          f\" {frame_index+1}.\"\n",
        "      )\n",
        "    except json.JSONDecodeError as e:\n",
        "      print(\n",
        "          f\"  Error decoding JSON for frame {frame_index+1}: {e}. Appending\"\n",
        "          \" empty list.\"\n",
        "      )\n",
        "      analyzed_frames_data.append([])\n",
        "    except Exception as e:\n",
        "      print(\n",
        "          \"  An unexpected error occurred processing frame\"\n",
        "          f\" {frame_index+1} response: {e}. Appending empty list.\"\n",
        "      )\n",
        "      analyzed_frames_data.append([])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\n",
        "        f\"  Error generating content for frame {frame_index+1}: {e}. Appending\"\n",
        "        \" empty list.\"\n",
        "    )\n",
        "    analyzed_frames_data.append([])\n",
        "\n",
        "\n",
        "print(f\"Collected point data for {len(analyzed_frames_data)} analyzed frames.\")\n",
        "\n",
        "points_data_all_frames = populate_points_for_all_frames(\n",
        "    len(frames), frame_step, analyzed_frames_data\n",
        ")\n",
        "print(f\"Populated point data for {len(points_data_all_frames)} total frames.\")\n",
        "\n",
        "\n",
        "modified_frames = overlay_points_on_frames(frames, points_data_all_frames)\n",
        "display_gif(modified_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK9IACHTeETV"
      },
      "source": [
        "## Object Detection and Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCgpTAPmeG7n"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha-arms-table.png -O aloha-arms-table.png -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGGCbTnOjkJx"
      },
      "source": [
        "### 2D Bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ekqe-vceI5O"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"aloha-arms-table.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "      Return bounding boxes as a JSON array with labels. Never return masks or\n",
        "      code fencing. Limit to 25 objects. Include as many objects as you can\n",
        "      identify on the table.\n",
        "      If an object is present multiple times, name them according to their\n",
        "      unique characteristic (colors, size, position, unique characteristics,\n",
        "      etc..).\n",
        "      The format should be as follows:\n",
        "      [{\"box_2d\": [ymin, xmin, ymax, xmax], \"label\": <label for the object>}]\n",
        "      normalized to 0-1000. The values in box_2d must only be integers.\n",
        "\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "\n",
        "plot_bounding_boxes(img, json_output)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9n_0kzeRsb"
      },
      "source": [
        "## Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzUkGwxGeTok"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/aloha_desk.png -O aloha_desk.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/particles.jpg -O particles.jpg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/livingroom.jpeg -O livingroom.jpeg -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJXZTJYojnu7"
      },
      "outputs": [],
      "source": [
        "### Simple Trajectory Planning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4no_6HBeU-j"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"aloha_desk.png\")\n",
        "points_data = []\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Place a point on the red pen, then 15 points for the trajectory of moving\n",
        "    the red pen to the top of the organizer on the left.\n",
        "\n",
        "    The points should be labeled by order of the trajectory, from '0' (start\n",
        "    point at left hand) to <n> (final point).\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  points_data.extend(data)\n",
        "except json.JSONDecodeError:\n",
        "  print(\"Warning: Invalid JSON response. Skipping.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN3aS0kQeXfm"
      },
      "outputs": [],
      "source": [
        "# @title Path for Brushing Particles\n",
        "\n",
        "img = get_image_resized(\"particles.jpg\")\n",
        "points_data = []\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Point to the the blue brush and a list of 10 points covering the region of\n",
        "    particles. Ensure that the points are spread evenly over the particles to\n",
        "    create a smooth trajectory.\n",
        "\n",
        "    Label the points from 1 to 10 based on the order that they should be\n",
        "    approached in the trajectory of cleaning the plate. Movement should start\n",
        "    from the brush.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  points_data.extend(data)\n",
        "except json.JSONDecodeError:\n",
        "  print(\"Warning: Invalid JSON response. Skipping.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KH4E1Zsjqb-"
      },
      "source": [
        "### Obstacle-avoidance trajectory planning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwdO5V2cPkEE"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"livingroom.jpeg\")\n",
        "points_data = []\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Find the most direct collision-free trajectory of 10 points on the floor\n",
        "    between the current view origin and the green ottoman in the back left.\n",
        "    The points should avoid all other obstacles on the floor.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\n",
        "    \"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "try:\n",
        "  data = json.loads(json_output)\n",
        "  points_data.extend(data)\n",
        "except json.JSONDecodeError:\n",
        "  print(\"Warning: Invalid JSON response. Skipping.\")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json.dumps(points_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y74wtNdpeqRE"
      },
      "source": [
        "## Spatial Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhP3JnXResEw"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/clear_space.png -O clear_space.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/desk_organization.mp4 -O desk_organization.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/bookshelf.jpeg -O bookshelf.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/cart.png -O cart.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/sockets.jpeg -O sockets.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/weights.jpeg -O weights.jpeg -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/lunch.png -O lunch.png -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUq7demFjzEd"
      },
      "source": [
        "### Item to remove to make room for laptop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU4ZTwgdXQeb"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"clear_space.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Point to the object that I need to remove to make room for my laptop.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "config=types.GenerateContentConfig(\n",
        "    temperature=0.5,\n",
        "    thinking_config=types.ThinkingConfig(thinking_budget=1024),\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vqSTmSCj2fG"
      },
      "source": [
        "### Orchestrating: Packing a Lunch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S0jeUGS8Yik"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"lunch.png\")\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Explain how to pack the lunch box and lunch bag. Point to each object that\n",
        "    you refer to.\n",
        "\n",
        "    Each point should be in the format:\n",
        "    [{\"point\": [y, x], \"label\": }]\n",
        "    where the coordinates are normalized between 0-1000.\n",
        "    \"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juA_MYjXj8I-"
      },
      "source": [
        "### Empty electrical sockets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN9763Z452dG"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"sockets.jpeg\")\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Point to the unobstructed empty sockets.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlRXLsF1j_aD"
      },
      "source": [
        "### Limiting item lift (3LB limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HV4SOPS3Jc3"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"weights.jpeg\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    I am a robot with a payload of 3LBs. Point to all the objects in the image I\n",
        "    am physically able to pick up.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(\n",
        "    temperature=0.5,\n",
        "    thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
        ")\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfTp_GvQcc8Y"
      },
      "source": [
        "### Video Analysis\n",
        "\n",
        "<video controls src=\"https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/desk_organization.mp4\" width=600px>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NJNi60l6UlK"
      },
      "outputs": [],
      "source": [
        "myfile = client.files.upload(file=\"/content/desk_organization.mp4\")\n",
        "while myfile.state == \"PROCESSING\":\n",
        "  print(\".\", end=\"\")\n",
        "  time.sleep(1)\n",
        "  myfile = client.files.get(name=myfile.name)\n",
        "\n",
        "if myfile.state.name == \"FAILED\":\n",
        "  raise ValueError(myfile.state.name)\n",
        "\n",
        "print(\"Uploaded\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Describe in detail each step of finishing the task. Breaking it down by\n",
        "    timestamp, output in JSON format with keys \"start_timestamp\",\n",
        "    \"end_timestamp\" and \"description\".\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[myfile, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.5,\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
        "    ),\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"\\nTotal processing time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6By4EZl2kECL"
      },
      "source": [
        "### Video Analysis: Time Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eXAWLyiaiXy"
      },
      "outputs": [],
      "source": [
        "conversation_history = [\n",
        "    {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
        "    {\"role\": \"model\", \"parts\": [{\"text\": response.text}]},\n",
        "]\n",
        "\n",
        "chat = client.chats.create(model=MODEL_ID, history=conversation_history)\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Zoom into second 15 to 22 and provide a per-second breakdown of what is\n",
        "    happening in the same format.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "response = chat.send_message([prompt, myfile])\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"\\nTotal processing time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsA-W2qzkNoX"
      },
      "source": [
        "### Finding the fourth row of shelves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7jL2EKfe20o"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"bookshelf.jpeg\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Return bounding boxes as a JSON array with labels highlighting all cubbies\n",
        "    in the fourth row of shelves.\n",
        "\n",
        "    The format should be as follows:\n",
        "    [{\"box_2d\": [ymin, xmin, ymax, xmax], \"label\": <label for the object>}]\n",
        "\n",
        "    normalized to 0-1000. The values in box_2d must only be integers.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "plot_bounding_boxes(img, json_output)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70jGus3kQFA"
      },
      "source": [
        "### Finding shelves with specific items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7uOXX7Ce4Ot"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"bookshelf.jpeg\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    \"I need to blow my nose.\"\n",
        "    Find the cubby that can help.\n",
        "\n",
        "    The format should be as follows:\n",
        "    [{\"box_2d\": [ymin, xmin, ymax, xmax], \"label\": <label for the object>}]\n",
        "\n",
        "    normalized to 0-1000. The values in box_2d must only be integers.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "plot_bounding_boxes(img, json_output)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6Jv0NqkTzq"
      },
      "source": [
        "### Counting items with thinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGTa8Jvdw3AR"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"cart.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    How many items are inside of the cart basket?\n",
        "    Please share your reasoning.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMwl91mQBgqe"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_1.png -O initial_state_1.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_2.png -O initial_state_2.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_3.png -O initial_state_3.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/initial_state_4.png -O initial_state_4.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_1.png -O current_state_1.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_2.png -O current_state_2.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_3.png -O current_state_3.png -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/current_state_4.png -O current_state_4.png -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrfTocxhs_Rj"
      },
      "source": [
        "### Multi-view correspondence and Success Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O57mlUujulDq"
      },
      "outputs": [],
      "source": [
        "initial_state_1 = Image.open(\"initial_state_1.png\")\n",
        "initial_state_2 = Image.open(\"initial_state_2.png\")\n",
        "initial_state_3 = Image.open(\"initial_state_3.png\")\n",
        "initial_state_4 = Image.open(\"initial_state_4.png\")\n",
        "current_state_1 = Image.open(\"current_state_1.png\")\n",
        "current_state_2 = Image.open(\"current_state_2.png\")\n",
        "current_state_3 = Image.open(\"current_state_3.png\")\n",
        "current_state_4 = Image.open(\"current_state_4.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    For this task, you will see a robot or human trying to perform the task of\n",
        "    putting the mango into the brown container. You may see multiple camera\n",
        "    views of the same scene. Some cameras are static and are mounted outside of\n",
        "    the scene and some cameras are mounted on the robot arms and thus they are\n",
        "    moving during the episode.\n",
        "\n",
        "    The first 4 images show multiple camera views from the start of the episode\n",
        "    (some time ago). The last 4 images show multiple camera views from the\n",
        "    current moment in the episode (as it is now).\n",
        "\n",
        "    Looking at these images and comparing the start of the episode with current\n",
        "    state did the robot successfully perform the task \"put the mango into the\n",
        "    brown container\"?\n",
        "\n",
        "    Answer only with (1) yes or (2) no. Return the number (1) or (2) that best\n",
        "    answers the question.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        initial_state_1,\n",
        "        initial_state_2,\n",
        "        initial_state_3,\n",
        "        initial_state_4,\n",
        "        current_state_1,\n",
        "        current_state_2,\n",
        "        current_state_3,\n",
        "        current_state_4,\n",
        "        prompt\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(temperature=0.5),\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTotal processing time: {elapsed_time:.4f} seconds\")\n",
        "print(f\"Success? {'Yes' if response.text == '(1)' else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_bjl2oglZPY"
      },
      "source": [
        "## Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaAEwd8j4Ki8"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/air_quality.jpeg -O air_quality.jpeg -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycdxXZZikW83"
      },
      "source": [
        "### Zooming in on sections of an image for better readings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5RGtAUhBbp"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"air_quality.jpeg\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    What is the air quality reading? Using the code execution feature, zoom in\n",
        "    on the image to take a closer look.\"\"\")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[img, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.5,\n",
        "        tools=[types.Tool(code_execution=types.ToolCodeExecution)],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "  if part.text is not None:\n",
        "    print(part.text)\n",
        "  if part.executable_code is not None:\n",
        "    print(part.executable_code.code)\n",
        "  if part.code_execution_result is not None:\n",
        "    print(part.code_execution_result.output)\n",
        "\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD5cDm3jCrwH"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIYq0-LSCvw8"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/mango.png -O mango.png -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwttN_tTkaq8"
      },
      "source": [
        "### Segmentation with robot gripper and item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaCtmnp1UOMp"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"mango.png\")\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Provide the segmentation masks for the following objects in this image:\n",
        "    mango, left robot gripper finger, right robot gripper finger.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [\n",
        "      {\n",
        "        \"box_2d\": [ymin, xmin, ymax, xmax],\n",
        "        \"label\": \"<label for the object>\",\n",
        "        \"mask\": \"data:image/png;base64,<base64 encoded PNG mask>\"\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "\n",
        "    The box_2d coordinates should be normalized to 0-1000 and must be integers.\n",
        "    The mask should be a base64 encoded PNG image where non-zero pixels indicate\n",
        "    the mask.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "config=types.GenerateContentConfig(temperature=0.5)\n",
        "print(\"Raw Model Response Text:\")\n",
        "json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "\n",
        "try:\n",
        "  segmentation_masks = parse_segmentation_masks(\n",
        "      json_output, img_height=img.size[1], img_width=img.size[0]\n",
        "  )\n",
        "  print(f\"Successfully parsed {len(segmentation_masks)} segmentation masks.\")\n",
        "\n",
        "  annotated_img = plot_segmentation_masks(\n",
        "      img.convert(\"RGBA\"), segmentation_masks\n",
        "  )\n",
        "  display.display(annotated_img)\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "  print(f\"Error decoding JSON response: {e}\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during mask processing or plotting: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MQzDeWjJVpu"
      },
      "source": [
        "## Task Orchestration\n",
        "\n",
        "To demonstrate task orchestration with your own custom robot API, this example will introduce a mock API that can be used for a simple pick-and-place operation. Both the block and the container where the block should be placed will be located and highlighted, and then the series of functions provided to the API will be called with appropriate logic for performing the action. This will use an origin system with 0,0 in a corner rather than on the robot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNYGjW5rJy3c"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/soarm-block.png -O soarm-block.png -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh7nSyWfkhNN"
      },
      "source": [
        "### Locate relevant objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRTLVVEJJYoI"
      },
      "outputs": [],
      "source": [
        "img = get_image_resized(\"soarm-block.png\")\n",
        "points_data = []\n",
        "\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Locate and point to the blue block and the orange bowl. The label returned\n",
        "    should be an identifying name for the object detected.\n",
        "\n",
        "    The answer should follow the JSON format:\n",
        "    [{\"point\": <point>, \"label\": <label1>}, ...]\n",
        "\n",
        "    The points are in [y, x] format normalized to 0-1000.\"\"\")\n",
        "\n",
        "start_time = time.time()\n",
        "json_output = call_gemini_robotics_er(img, prompt)\n",
        "\n",
        "data = json.loads(json_output)\n",
        "points_data.extend(data)\n",
        "\n",
        "print(f\"\\nTotal processing time: {(time.time() - start_time):.4f} seconds\")\n",
        "IPython.display.HTML(generate_point_html(img, json_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGiPj_kkkkX"
      },
      "source": [
        "### Use location of relevant objects to call functions and perform pick-and-place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChFtc8PhSs64"
      },
      "outputs": [],
      "source": [
        "# Define the robot's origin point so coordinates can be based on robot location\n",
        "# for movements rather than original origin from the corner.\n",
        "\n",
        "def move(x, y, high):\n",
        "  print(f\"moving to coordinates: {x}, {y}, {15 if high else 5}\")\n",
        "\n",
        "\n",
        "def setGripperState(opened):\n",
        "  print(\"Opening gripper\" if opened else \"Closing gripper\")\n",
        "\n",
        "\n",
        "def returnToOrigin():\n",
        "  print(\"Returning to origin pose\")\n",
        "\n",
        "\n",
        "robot_origin_y = 300\n",
        "robot_origin_x = 500\n",
        "\n",
        "blue_block_point = None\n",
        "orange_bowl_point = None\n",
        "\n",
        "for item in points_data:\n",
        "  if item.get(\"label\") == \"blue block\":\n",
        "    blue_block_point = item.get(\"point\")\n",
        "  elif item.get(\"label\") == \"orange bowl\":\n",
        "    orange_bowl_point = item.get(\"point\")\n",
        "\n",
        "if blue_block_point and orange_bowl_point:\n",
        "  block_y, block_x = blue_block_point\n",
        "  bowl_y, bowl_x = orange_bowl_point\n",
        "\n",
        "  print(f\"Blue block normalized coordinates (y, x): {block_y}, {block_x}\")\n",
        "  print(f\"Orange bowl normalized coordinates (y, x): {bowl_y}, {bowl_x}\")\n",
        "\n",
        "  block_relative_x = block_x - robot_origin_x\n",
        "  block_relative_y = block_y - robot_origin_y\n",
        "  bowl_relative_x = bowl_x - robot_origin_x\n",
        "  bowl_relative_y = bowl_y - robot_origin_y\n",
        "\n",
        "  prompt = textwrap.dedent(f\"\"\"\\\n",
        "      You are a robotic arm with six degrees-of-freedom. You have the following\n",
        "      functions available to you:\n",
        "\n",
        "      def move(x, y, high):\n",
        "        # Moves the arm to the given coordinates. The boolean value 'high' set\n",
        "        # to True means the robot arm should be lifted above the scene for\n",
        "        # avoiding obstacles during motion. 'high' set to False means the robot\n",
        "        # arm should have the gripper placed on the surface for interacting with\n",
        "        # objects.\n",
        "\n",
        "      def setGripperState(opened):\n",
        "        # Opens the gripper if opened set to true, otherwise closes the gripper\n",
        "\n",
        "      def returnToOrigin():\n",
        "        # Returns the robot to an initial state. Should be called as a cleanup\n",
        "        # operation.\n",
        "\n",
        "      The origin point for calculating the moves is at normalized point\n",
        "      y={robot_origin_y}, x={robot_origin_x}. Use this as the new (0,0) for\n",
        "      calculating moves, allowing x and y to be negative.\n",
        "\n",
        "      Perform a pick and place operation where you pick up the blue block at\n",
        "      normalized coordinates ({block_x}, {block_y}) (relative coordinates:\n",
        "      {block_relative_x}, {block_relative_y}) and place it into the orange bowl\n",
        "      at normalized coordinates ({bowl_x}, {bowl_y}) (relative coordinates:\n",
        "      {bowl_relative_x}, {bowl_relative_y}).\n",
        "      Provide the sequence of function calls as a JSON list of objects, where\n",
        "      each object has a \"function\" key (the function name) and an \"args\" key\n",
        "      (a list of arguments for the function).\n",
        "\n",
        "      Also, include your reasoning before the JSON output.\n",
        "\n",
        "      For example:\n",
        "      Reasoning: To pick up the block, I will first move the arm to a high\n",
        "      position above the block, open the gripper, move down to the block, close\n",
        "      the gripper, lift the arm, move to a high position above the bowl, move\n",
        "      down to the bowl, open the gripper, and then lift the arm back to a high\n",
        "      position.\"\"\")\n",
        "\n",
        "  start_time = time.time()\n",
        "  config=types.GenerateContentConfig(temperature=0.5)\n",
        "  print(\"Model Response:\")\n",
        "  json_output = call_gemini_robotics_er(img, prompt, config)\n",
        "\n",
        "  try:\n",
        "    function_calls = json.loads(json_output)\n",
        "\n",
        "    print(\"\\nExecuting Function Calls:\")\n",
        "    for call in function_calls:\n",
        "      function_name = call.get(\"function\")\n",
        "      arguments = call.get(\"args\", [])\n",
        "\n",
        "      if function_name == \"move\":\n",
        "        move(*arguments)\n",
        "      elif function_name == \"setGripperState\":\n",
        "        setGripperState(*arguments)\n",
        "      elif function_name == \"returnToOrigin\":\n",
        "        returnToOrigin()\n",
        "      else:\n",
        "        print(f\"Unknown function: {function_name}\")\n",
        "\n",
        "  except json.JSONDecodeError:\n",
        "    print(\"Error: Could not parse JSON response from the model.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during function execution: {e}\")\n",
        "\n",
        "else:\n",
        "  print(\"Could not find coordinates for both blue block and orange bowl.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoRvpMfk--6"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "For more details on the Gemini Robotics-ER 1.5 model, see the [documentation](https://ai.google.dev/gemini-api/docs/robotics-overview)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XRlsn7tJYUjI",
        "VuvfFW2sa1wu"
      ],
      "name": "gemini-robotics-er.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}