{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGdicu8PVD9"
      },
      "source": [
        "# Use Gemini thinking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4Ti6Q0QKIl"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w14yjWnPVD-"
      },
      "source": [
        "[Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-04-17) and [Gemini 2.5 Pro](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-06-05) are models that are trained to do a [thinking process](https://ai.google.dev/gemini-api/docs/thinking-mode) (or reasoning) before getting to a final answer. As a result,\n",
        "those models are capable of stronger reasoning capabilities in its responses than previous models.\n",
        "\n",
        "You'll see examples of those reasoning capabilities with [code understanding](#scrollTo=GAa7sCD7tuMW), [geometry](#scrollTo=ADiJV-fFyjRe) and [math](#scrollTo=EXPPWpt6ttJZ) problems.\n",
        "\n",
        "As you will see, the model is exposing its thoughts so you can have a look at its reasoning and how it did reach its conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsG7Z-t1AoP"
      },
      "source": [
        "## Understanding the thinking models\n",
        "\n",
        "[Gemini 2.5 models](https://ai.google.dev/gemini-api/docs/thinking) are optimized for complex tasks that need multiple rounds of strategyzing and iteratively solving.\n",
        "\n",
        "[Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-04-17) in particular, brings the flexibility of using `thinking_budget` - a parameter\n",
        "that offers fine-grained control over the maximum number of tokens a model can generate while thinking. Alternatively, you can designate a precise token allowance for the\n",
        "\"thinking\" stage through the adjusment of the `thinking_budget` parameter. This allowance can vary between 0 and 24576 tokens for 2.5 Flash.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them.\n",
        "\n",
        "On this notebook all examples are using `Gemini 2.5 Pro` and `Gemini 2.5 Flash` with the new `thinking_budget` parameter. For more information about using the `thinking_budget` with the Gemini thinking model, check the [documentation](https://ai.google.dev/gemini-api/docs/thinking)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0HWzIEAQYqz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section install the SDK, set it up using your [API key](../quickstarts/Authentication.ipynb), imports the relevant libs, downloads the sample videos and upload them to Gemini.\n",
        "\n",
        "Just collapse (click on the little arrow on the left of the title) and run this section if you want to jump straight to the examples (just don't forget to run it otherwise nothing will work)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzBKAaL4QYq0"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://ai.google.dev/gemini-api/docs/sdks)** provides programmatic access to Gemini models using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IbKkL5ksQYq1"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.16.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUGen_kQYq2"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0H_lRdlrQYq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "6c0aed64-e483-472b-b136-ec6b68cd3686"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GOOGLE_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3353361709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GOOGLE_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Lez1vBQYq3"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3CAp9YrQYq4"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNIfaceKFC82"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash-lite-preview-09-2025\", \"gemini-2.5-flash\", \"gemini-2.5-flash-preview-09-2025\", \"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5tDbb-Q0oc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Z9QzC3Q2wX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAPiYdYMfeJP"
      },
      "source": [
        "## Using the thinking models\n",
        "\n",
        "Here are some quite complex examples of what Gemini thinking models can solve.\n",
        "\n",
        "In each of them you can select different models to see how this new model compares to its predecesors.\n",
        "\n",
        "In some cases, you'll still get the good answer from the other models, in that case, re-run it a couple of times and you'll see that Gemini thinking models are more consistent thanks to their thinking step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D30HsUJ2x54"
      },
      "source": [
        "### Using adaptive thinking\n",
        "\n",
        "You can start by asking the model to explain a concept and see how it does reasoning before answering.\n",
        "\n",
        "Starting with the adaptive `thinking_budget` - which is the default when you don't specify a budget - the model will dynamically adjust the budget based on the complexity of the request.\n",
        "\n",
        "The animal it should find is a [**Platipus**](https://en.wikipedia.org/wiki/Platypus), but as you'll see it is not the first answer it thinks of depending on how much thinking it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffa2fd81d26e"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    You are playing the 20 question game. You know that what you are looking for\n",
        "    is a aquatic mammal that doesn't live in the sea, is venomous and that's\n",
        "    smaller than a cat. What could that be and how could you make sure?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf57b6797848"
      },
      "source": [
        "Looking to the response metadata, you can see not only the amount of tokens on your input and the amount of tokens used for the response, but also the amount of tokens used for the thinking step - As you can see here, the model used around 1400 tokens in the thinking steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d09a98f06e6"
      },
      "outputs": [],
      "source": [
        "print(\"Prompt tokens:\",response.usage_metadata.prompt_token_count)\n",
        "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
        "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)\n",
        "print(\"Total tokens:\",response.usage_metadata.total_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aecf74aff72"
      },
      "source": [
        "### Disabling the thinking steps\n",
        "\n",
        "You can also disable the thinking steps by setting the `thinking_budget` to 0. You'll see that in this case, the model doesn't think of the platipus as a possible answer.\n",
        "\n",
        "**NOTE:** For now, you can disable the thinking steps when using the `gemini-2.5-flash-preview-05-20` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602bf11f5b78"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    You are playing the 20 question game. You know that what you are looking for\n",
        "    is a aquatic mammal that doesn't live in the sea, is venomous and that's\n",
        "    smaller than a cat. What could that be and how could you make sure?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=prompt,\n",
        "  config=types.GenerateContentConfig(\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      thinking_budget=0\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16dfc2a51e4d"
      },
      "source": [
        "Now you can see that the response is faster as the model didn't perform any thinking step. Also you can see that no tokens were used for the thinking step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dcd275d0b42"
      },
      "outputs": [],
      "source": [
        "print(\"Prompt tokens:\",response.usage_metadata.prompt_token_count)\n",
        "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
        "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)\n",
        "print(\"Total tokens:\",response.usage_metadata.total_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAa7sCD7tuMW"
      },
      "source": [
        "### Solving a physics problem\n",
        "\n",
        "Now, try with a simple physics comprehension example. First you can disable the `thinking_budget` to see how the model performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZw41-lsKKMf"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n",
        "    It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n",
        "    Calculate the maximum bending stress (σ_max).\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=0\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf87bae4035"
      },
      "source": [
        "You can see that the model used no tokens for the thinking step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b59d32afd90"
      },
      "outputs": [],
      "source": [
        "print(\"Prompt tokens:\",response.usage_metadata.prompt_token_count)\n",
        "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
        "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)\n",
        "print(\"Total tokens:\",response.usage_metadata.total_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14873702305e"
      },
      "source": [
        "Then you can set a fixed maximum budget (`thinking_budget=4096`, or 4096 tokens) for the thinking step to see how the model performs.\n",
        "\n",
        "You can see that, even producing a similar result for the same prompt, the amount of details shared in the answer makes it deeper and more consistent.\n",
        "\n",
        "**NOTE:** You have different possible budget values for 2.5 Pro and 2.5 Flash:\n",
        "- for the Gemini 2.5 Pro, the budgets can be between `128` and `32768`\n",
        "- for the Gemini 2.5 Flash, the budgets can be between `0` (disabling the thinking process) to `24576`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1783ef351094"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    A cantilever beam of length L=3m has a rectangular cross-section (width b=0.1m, height h=0.2m) and is made of steel (E=200 GPa).\n",
        "    It is subjected to a uniformly distributed load w=5 kN/m along its entire length and a point load P=10 kN at its free end.\n",
        "    Calculate the maximum bending stress (σ_max).\n",
        "\"\"\"\n",
        "\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe9bafe22c64"
      },
      "source": [
        "Now you can see that the model used around 2000 tokens for the thinking step (not necessarily using the full budget you set):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dfac516c27b"
      },
      "outputs": [],
      "source": [
        "print(\"Prompt tokens:\",response.usage_metadata.prompt_token_count)\n",
        "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count,\"/\",thinking_budget)\n",
        "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)\n",
        "print(\"Total tokens:\",response.usage_metadata.total_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTuVuVeAnUVR"
      },
      "source": [
        "Keep in mind that the largest the thinking budget is, the longest the model will spend time thinking, with means a longer computation time and a more expensive request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADiJV-fFyjRe"
      },
      "source": [
        "### Working with multimodal problems\n",
        "\n",
        "This geometry problem requires complex reasoning and is also using Gemini multimodal abilities to read the image.\n",
        "In this case, you are fixing a value to the `thinking_budget` so the model will use up to 8196 tokens for the thinking step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcXWXqyzCjQ"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/geometry.png -O geometry.png -q\n",
        "\n",
        "im = Image.open(\"geometry.png\").resize((256,256))\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb9o7AeDwVyZ"
      },
      "outputs": [],
      "source": [
        "prompt = \"What's the area of the overlapping region?\"\n",
        "\n",
        "thinking_budget = 8192 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[im, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPPWpt6ttJZ"
      },
      "source": [
        "### Solving brain teasers\n",
        "\n",
        "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the toughts of the model you'll see that it will realize it and come up with an out-of-the-box solution.\n",
        "\n",
        "In this case, you are fixing a value to the `thinking_budget` so the model will use up to 24576 tokens for the thinking step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2YeBqzC0J_i"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/generativeai-downloads/images/pool.png -O pool.png -q\n",
        "\n",
        "im = Image.open(\"pool.png\").resize((256,256))\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDNOYONW5u7z"
      },
      "source": [
        "First you can check how the model performs without reasoning (`thinking_budget=0`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt2dSjeqA9ZC"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        im,\n",
        "        \"How do I use those three pool balls to sum up to 30?\"\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=0\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1c6oyBU54XI"
      },
      "source": [
        "As you can notice, the model struggled to find a way to get to the result - and ended up suggesting to use different pool balls.\n",
        "\n",
        "Now you can use the model reasoning to solve the riddle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fnVAJuv530a"
      },
      "outputs": [],
      "source": [
        "prompt = \"How do I use those three pool balls to sum up to 30?\"\n",
        "\n",
        "thinking_budget = 24576 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        im,\n",
        "        prompt,\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "511ffa6a2c5c"
      },
      "source": [
        "### Solving a math puzzle with the maximum `thinking_budget`\n",
        "\n",
        "This is typically a case where you want to fix a budget, as the model can spend a lot of time thinking in all directions before finding the right answer. It should not be too low either as non-thinking models have trouble with such questions.\n",
        "\n",
        "Play with the thinking budget and try to find how much it needs to be able to find the right answer most of the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91072ecec725"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "   How can you obtain 565 with 10 8 3 7 1 and 5 and the common operations?\n",
        "   You can only use a number once.\n",
        "\"\"\"\n",
        "\n",
        "thinking_budget = 24576 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1teR7Fx6_Zh"
      },
      "source": [
        "## Working thoughts summaries\n",
        "\n",
        "Summaries of the model's thinking reveal its internal problem-solving pathway. Users can leverage this feature to check the model's strategy and remain informed during complex tasks.\n",
        "\n",
        "For more details about Gemini 2.5 thinking capabilities, take a look at the [Gemini models thinking guide](https://googledevai.devsite.corp.google.com/gemini-api/docs/thinking#summaries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtEhB8xC8QAF"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n",
        "  The person who lives in the red house owns a cat.\n",
        "  Bob does not live in the green house.\n",
        "  Carol owns a dog.\n",
        "  The green house is to the left of the red house.\n",
        "  Alice does not own a cat.\n",
        "  Who lives in each house, and what pet do they own?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=prompt,\n",
        "  config=types.GenerateContentConfig(\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      include_thoughts=True\n",
        "    )\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39IeBCYv8PR7"
      },
      "source": [
        "You can check both the thought summaries and the final model response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcgYIrda8kAo"
      },
      "outputs": [],
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  if not part.text:\n",
        "    continue\n",
        "  elif part.thought:\n",
        "    display(Markdown(\"## **Thoughts summary:**\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "  else:\n",
        "    display(Markdown(\"## **Answer:**\"))\n",
        "    display(Markdown(part.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7MkrNn28vxY"
      },
      "source": [
        "You can also use see the thought summaries in streaming experiences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93S2-d5X85kq"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n",
        "  The person who lives in the red house owns a cat.\n",
        "  Bob does not live in the green house.\n",
        "  Carol owns a dog.\n",
        "  The green house is to the left of the red house.\n",
        "  Alice does not own a cat.\n",
        "  Who lives in each house, and what pet do they own?\n",
        "\"\"\"\n",
        "\n",
        "thoughts = \"\"\n",
        "answer = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "      thinking_config=types.ThinkingConfig(\n",
        "        include_thoughts=True\n",
        "      )\n",
        "    )\n",
        "):\n",
        "  for part in chunk.candidates[0].content.parts:\n",
        "    if not part.text:\n",
        "      continue\n",
        "    elif part.thought:\n",
        "      if not thoughts:\n",
        "        display(Markdown(\"## **Thoughts summary:**\"))\n",
        "      display(Markdown(part.text.strip()))\n",
        "      thoughts += part.text\n",
        "    else:\n",
        "      if not answer:\n",
        "        display(Markdown(\"## **Answer:**\"))\n",
        "      display(Markdown(part.text.strip()))\n",
        "      answer += part.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH0Fq3AA_A0y"
      },
      "source": [
        "## Working with Gemini thinking models and tools\n",
        "\n",
        "Gemini thinking models are compatible with the tools and capabilities inherent to the Gemini ecosystem. This compatibility allows them to interface with external environments, execute computational code, or retrieve real-time data, subsequently incorporating such information into their analytical framework and concluding statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027fe5edcb8e"
      },
      "source": [
        "### Solving a problem using the code execution tool\n",
        "\n",
        "This example shows how to use the [code execution](./Code_execution.ipynb) tool to solve a problem. The model will generate the code and then execute it to get the final answer.\n",
        "\n",
        "In this case, you are using the adaptive thinking_budget so the model will dynamically adjust the budget based on the complexity of the request.\n",
        "\n",
        "If you want to experiment with a fixed budget, you can set the `thinking_budget` to a specific value (e.g. `thinking_budget=4096`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpFo0Q1Q_10o"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    What are the best ways to sort a list of n numbers from 0 to m?\n",
        "    Generate and run Python code for three different sort algorithms.\n",
        "    Provide the final comparison between algorithm clearly.\n",
        "    Is one of them linear?\n",
        "\"\"\"\n",
        "\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "code_execution_tool = types.Tool(\n",
        "    code_execution=types.ToolCodeExecution()\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget,\n",
        "        )\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjXgY2wg_-g9"
      },
      "source": [
        "Checking the model response, including the code generated and the execution result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KelkCAIdkzTW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "  if part.text is not None:\n",
        "    display(Markdown(part.text))\n",
        "  if part.executable_code is not None:\n",
        "    code_html = f'<pre style=\"background-color: green;\">{part.executable_code.code}</pre>' # Change code color\n",
        "    display(HTML(code_html))\n",
        "  if part.code_execution_result is not None:\n",
        "    display(Markdown(part.code_execution_result.output))\n",
        "display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhvYU0aVvhmM"
      },
      "source": [
        "### Thinking with search tool\n",
        "\n",
        "Search grounding is a great way to improve the quality of the model responses by giving it the ability to search for the latest information using Google Search. Check the [dedicated guide](./Search_Grounding.ipynb) for more details on that feature.\n",
        "\n",
        "In this case, you are using the adaptive thinking_budget so the model will dynamically adjust the budget based on the complexity of the request.\n",
        "\n",
        "If you want to experiment with a fixed budget, you can set the `thinking_budget` to a specific value (e.g. `thinking_budget=4096`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFp3r_dpv5di"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
        "\n",
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "prompt = \"\"\"\n",
        "    What were the major scientific breakthroughs announced last month? Use your\n",
        "    critical thinking and only list what's really incredible and not just an\n",
        "    overinfluated title.\n",
        "\"\"\"\n",
        "\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget,\n",
        "            include_thoughts=True\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88eLYC-QBHcn"
      },
      "source": [
        "Then you can check all information:\n",
        "- the model thoughts summary\n",
        "- the model answer\n",
        "- and the Google Search reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kEjC0VPmGob"
      },
      "outputs": [],
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  if not part.text:\n",
        "    continue\n",
        "  elif part.thought:\n",
        "    display(Markdown(\"## **Thoughts summary:**\"))\n",
        "    display(Markdown(part.text))\n",
        "    print()\n",
        "  else:\n",
        "    display(Markdown(\"## **Answer:**\"))\n",
        "    display(Markdown(part.text))\n",
        "\n",
        "display(Markdown(\"## **Google Search information:**\"))\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lND4jB6MrsSk"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "Try Gemini 2.5 Pro in\n",
        "[Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro), and learn more about [Prompting for thinking models](https://ai.google.dev/gemini-api/docs/thinking#best-practices).\n",
        "\n",
        "For more examples of the Gemini capabilities, check the other [Cookbook examples](https://github.com/google-gemini/cookbook). You'll learn how to use the [Live API](./Get_started.ipynb), juggle with [multiple tools](../examples/LiveAPI_plotting_and_mapping.ipynb) or use Gemini [spatial understanding](./Spatial_understanding.ipynb) abilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Get_started_thinking.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}